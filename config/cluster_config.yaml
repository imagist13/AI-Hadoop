# BigData Agent 集群连接配置
# 用于连接虚拟机节点和大数据集群

# =============================================================================
# 虚拟机节点配置
# =============================================================================
nodes:
  # 主节点 (NameNode, Master, etc.)
  node1:
    # 基本连接信息
    hostname: "node1"
    ip_address: "192.168.1.101"  # 根据实际IP修改
    port: 22
    username: "hadoop"  # 或其他用户，如root, ubuntu等
    password: null  # 密码认证（不推荐生产环境）
    key_file: "~/.ssh/id_rsa"  # SSH私钥路径
    key_passphrase: null  # 私钥密码（如果有）

    # 连接参数
    connection:
      timeout: 30  # 连接超时时间(秒)
      retry_count: 3  # 重试次数
      retry_delay: 5  # 重试间隔(秒)
      keep_alive: 60  # 保持连接时间(秒)

    # 服务端口配置
    services:
      ssh: 22
      hdfs_namenode: 9870  # HDFS NameNode Web UI
      yarn_resourcemanager: 8088  # YARN ResourceManager
      spark_master: 7077  # Spark Master
      hive_metastore: 9083  # Hive Metastore
      presto_coordinator: 8080  # Presto Coordinator
      clickhouse_http: 8123  # ClickHouse HTTP
      clickhouse_tcp: 9000  # ClickHouse TCP

    # 角色标识
    roles:
      - "namenode"
      - "resourcemanager"
      - "spark_master"
      - "hive_server"

    # 资源配置
    resources:
      memory_gb: 16
      cpu_cores: 8
      disk_gb: 500

  # 从节点示例 (可根据需要添加更多节点)
  node2:
    hostname: "node2"
    ip_address: "192.168.1.102"
    port: 22
    username: "hadoop"
    key_file: "~/.ssh/id_rsa"

    services:
      ssh: 22
      hdfs_datanode: 9864
      yarn_nodemanager: 8042
      spark_worker: 7078

    roles:
      - "datanode"
      - "nodemanager"
      - "spark_worker"

    resources:
      memory_gb: 32
      cpu_cores: 16
      disk_gb: 1000

  node3:
    hostname: "node3"
    ip_address: "192.168.1.103"
    port: 22
    username: "hadoop"
    key_file: "~/.ssh/id_rsa"

    services:
      ssh: 22
      hdfs_datanode: 9864
      yarn_nodemanager: 8042
      spark_worker: 7078

    roles:
      - "datanode"
      - "nodemanager"
      - "spark_worker"

    resources:
      memory_gb: 32
      cpu_cores: 16
      disk_gb: 1000

# =============================================================================
# 集群整体配置
# =============================================================================
cluster:
  # 集群基本信息
  name: "hadoop-cluster"
  version: "3.3.4"
  type: "hadoop"  # hadoop, spark, kubernetes等

  # 高可用配置
  high_availability:
    enabled: false
    namenode_standby: "node2"  # 备用NameNode

  # 网络配置
  network:
    domain: "cluster.local"
    subnet: "192.168.1.0/24"

  # 安全配置
  security:
    kerberos_enabled: false
    ldap_enabled: false

# =============================================================================
# BigData Agent 执行引擎配置
# =============================================================================
execution_engines:
  # Spark 配置
  spark:
    master_url: "spark://node1:7077"
    deploy_mode: "cluster"
    app_name: "BigDataAgent"

    # 默认资源配置
    default_resources:
      executor_memory: "2g"
      executor_cores: "2"
      num_executors: "2"
      driver_memory: "1g"

    # 队列配置
    queue: "default"

    # 高级配置
    configs:
      spark.sql.adaptive.enabled: "true"
      spark.sql.adaptive.coalescePartitions.enabled: "true"
      spark.serializer: "org.apache.spark.serializer.KryoSerializer"

  # Hive 配置
  hive:
    metastore_uri: "thrift://node1:9083"
    database: "default"
    auth_mechanism: "PLAIN"

  # Presto 配置
  presto:
    coordinator_uri: "http://node1:8080"
    catalog: "hive"
    schema: "default"

  # ClickHouse 配置
  clickhouse:
    host: "node1"
    http_port: 8123
    tcp_port: 9000
    database: "default"
    username: "default"
    password: ""

# =============================================================================
# 数据源配置
# =============================================================================
data_sources:
  # HDFS 配置
  hdfs:
    namenode_uri: "hdfs://node1:9000"
    user: "hadoop"
    replication: 3
    block_size: 134217728  # 128MB

  # 本地文件系统
  local:
    base_path: "/data"
    temp_path: "/tmp/bigdata_agent"

  # 对象存储 (可选)
  s3:
    endpoint: "https://s3.amazonaws.com"
    access_key: "${S3_ACCESS_KEY}"
    secret_key: "${S3_SECRET_KEY}"
    bucket: "bigdata-agent"
    region: "us-east-1"

# =============================================================================
# 监控和日志配置
# =============================================================================
monitoring:
  # 指标收集
  metrics:
    enabled: true
    interval_seconds: 60
    exporters:
      - type: "prometheus"
        endpoint: "http://localhost:9090"

  # 日志配置
  logging:
    level: "INFO"
    format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    handlers:
      - type: "file"
        path: "/var/log/bigdata-agent/agent.log"
        max_size_mb: 100
        backup_count: 5
      - type: "console"

# =============================================================================
# 安全配置
# =============================================================================
security:
  # SSH 配置
  ssh:
    strict_host_key_checking: false
    user_known_hosts_file: "~/.ssh/known_hosts"
    connect_timeout: 30

  # 加密配置
  encryption:
    enabled: false
    key_store_path: "/etc/bigdata-agent/keystore.jks"
    key_store_password: "${KEYSTORE_PASSWORD}"

  # 访问控制
  access_control:
    enabled: false
    admin_users:
      - "admin"
    readonly_users:
      - "viewer"

# =============================================================================
# 环境变量 (可选覆盖)
# =============================================================================
environment:
  # Java 环境
  JAVA_HOME: "/usr/lib/jvm/java-8-openjdk-amd64"
  HADOOP_HOME: "/opt/hadoop"
  SPARK_HOME: "/opt/spark"
  HIVE_HOME: "/opt/hive"

  # Python 环境
  PYTHONPATH: "/opt/bigdata-agent:$PYTHONPATH"

  # 其他环境变量
  HADOOP_CONF_DIR: "/opt/hadoop/etc/hadoop"
  SPARK_CONF_DIR: "/opt/spark/conf"
  HIVE_CONF_DIR: "/opt/hive/conf"

# =============================================================================
# 高级配置
# =============================================================================
advanced:
  # 连接池配置
  connection_pool:
    max_connections: 10
    max_keepalive_connections: 5
    keepalive_timeout: 600

  # 缓存配置
  cache:
    enabled: true
    ttl_seconds: 3600
    max_size_mb: 512

  # 性能调优
  performance:
    max_concurrent_queries: 5
    query_timeout_seconds: 3600
    result_batch_size: 1000

  # 调试配置
  debug:
    enabled: false
    log_queries: false
    profile_execution: false

# =============================================================================
# 注释和说明
# =============================================================================
# 修改说明:
# 1. 将所有IP地址修改为实际的虚拟机IP
# 2. 根据实际的Hadoop/Spark版本调整端口
# 3. 设置正确的用户名和SSH密钥路径
# 4. 根据集群规模调整资源配置
# 5. 生产环境建议启用安全配置
#
# 使用方法:
# python -c "import yaml; config = yaml.safe_load(open('config/cluster_config.yaml'))"
#
# 安全提醒:
# 1. 不要在配置文件中明文存储密码
# 2. 使用环境变量或密钥管理系统
# 3. 定期轮换SSH密钥和访问凭据
# 4. 限制配置文件访问权限

